{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "1. The script uses Python version=3.5.5, Keras=2.2.0, TensorFlow=1.8.0.\n",
    "1. Input data should be in the same parent folder as this python script. \n",
    "    Parent directory has input files. Its Sub-directory should have this python script.\n",
    "2. The code will create save_folder=\"CreditDeFaulter\", \n",
    "    containing Confusion Matrix/Output file reporting accuracies/ NN model checkpoints\n",
    "\n",
    "\"\"\"\n",
    "# will ignore unnecessary scikit-learn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "#from collections import Counter\n",
    "#import missingno as msno\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MaxAbsScaler \n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#from keras import regularizers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam #SGD,,RMSprop\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import xgboost\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "seed=1\n",
    "\n",
    "targetName, Tosave_Folder=\"loan_status\", \"Credit_Defaulter\"\n",
    "accuracies=[]\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(Tosave_Folder):\n",
    "    os.makedirs(Tosave_Folder)\n",
    "\n",
    "\n",
    "def is_File(fileNameInput):\n",
    "    \"\"\"\n",
    "    This function checks if the file exists.\n",
    "    Input - FileName\n",
    "    Output - raises Exception if couldnt fine it\n",
    "    \"\"\"\n",
    "    if(not os.path.isfile(fileNameInput)):\n",
    "        raise ValueError(\"You must provide a valid fileName as parameter\")\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\", fontsize='large')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plotting(dataset):\n",
    "    \"\"\"\n",
    "    This function does some feature inspection feature selection.\n",
    "    It's coded out as the output is shown in ppt file and not needed now.\n",
    "    \"\"\"\n",
    "    '''\n",
    "    # --Maximum amount of remaining outstanding principal for total amount funded\n",
    "    # --interest rate for most Defaulters\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    ax.set_xlabel('interest_rate')\n",
    "    ax.set_ylabel('out_prncp')\n",
    "    dataset.groupby(['interest_rate','loan_status']).count()['out_prncp'].unstack().plot(kind='bar',ax=ax,)\n",
    "    plt.show()\n",
    "    # --For term, max acc_now_delinq having loan status default.\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    ax.set_xlabel('term')\n",
    "    ax.set_ylabel('acc_now_delinq')\n",
    "    dataset.groupby(['term','loan_status']).count()['acc_now_delinq'].unstack().plot(kind='bar',ax=ax)\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Read_Data():        \n",
    "    \"\"\"\n",
    "    This function reads data from 3 input files\n",
    "    output - Dataset with filtered columns\n",
    "    \"\"\"\n",
    "    #---- Reading file in data frame\n",
    "    fileNameInput=[\"Loan Classification Information.csv\", \"Borrower Information.csv\",\"Payment.csv\"]\n",
    "\n",
    "    data_loan = pd.read_csv(\"../\"+str(fileNameInput[0]),header='infer', sep=',', encoding=\"utf-8\", error_bad_lines=False ,  doublequote=True, low_memory=False)    \n",
    "    data_borrow = pd.read_csv(\"../\"+str(fileNameInput[1]),header='infer', sep=',', encoding=\"utf-8\",error_bad_lines=False ,  doublequote=True, low_memory=False)\n",
    "    data_payments = pd.read_csv(\"../\"+str(fileNameInput[2]),header='infer', sep=',', encoding=\"utf-8\",error_bad_lines=False ,  doublequote=True,low_memory=False)\n",
    "   \n",
    "    \n",
    "    # Filter needed coloumns\n",
    "    data_loan_subset = data_loan[[\"funded_amnt\",\"funded_amnt_inv\", \"int_rate\", \"loan_amnt\",\n",
    "                                              \"loan_status\",\"pymnt_plan\",\"term\",\"sub_grade\"]]\n",
    "    data_borrow_subset = data_borrow[[\"annual_inc\",\"emp_length\",\n",
    "                                           \"open_acc\",\"pub_rec\",\"total_acc\"]]\n",
    "\n",
    "    data_payments_subset = data_payments[[\"last_pymnt_amnt\",\"revol_bal\",\"total_pymnt\",\"total_pymnt_inv\",\n",
    "                                      \"total_rec_late_fee\" ]]\n",
    "    \n",
    "    # Merging all selected features\n",
    "    data_subset=pd.concat([data_loan_subset,data_borrow_subset, data_payments_subset], axis=1)\n",
    "    print(\"Dataframe with filtered columns - shape is: \",np.shape(data_subset))\n",
    "    return data_subset\n",
    "\n",
    "\n",
    "\n",
    "def Under_Sampling(data_subset):\n",
    "    \"\"\" \n",
    "    This function deals with data imbalance.\n",
    "    inuput - Takes a dataset with filtered columns as input.\n",
    "    output - Balanced target classes dataset, and whole dataset (all Fully paid rows & Default) \n",
    "    \"\"\"\n",
    "\n",
    "    # keep only fully paid and Default data\n",
    "    fraud_indices =data_subset.index[data_subset['loan_status'].isin(['Default'])].tolist()\n",
    "    normal_indices =data_subset.index[data_subset['loan_status'].isin(['Fully Paid'])].tolist()\n",
    "\n",
    "    full_data_indices = np.concatenate([fraud_indices, normal_indices])\n",
    "    whole_data = data_subset.iloc[full_data_indices,:]\n",
    "    \n",
    "    count_Default = np.count_nonzero(fraud_indices)\n",
    "#    random_normal_indices = np.random.choice(normal_indices, count_Default+500, replace = False)\n",
    "\n",
    "    normal_indices = shuffle(np.array(normal_indices), random_state=seed)\n",
    "    random_normal_indices = normal_indices[:count_Default]\n",
    "    random_normal_indices = np.array(random_normal_indices)\n",
    "    \n",
    "    under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
    "\n",
    "    under_sample_data = data_subset.iloc[under_sample_indices,:]\n",
    "    \n",
    "    \n",
    "  \n",
    "\n",
    " \n",
    "    \n",
    "#    print (len(under_sample_data['loan_status'=='Default']))\n",
    "    print (under_sample_data.head(5))\n",
    "    print(\"Normal indices\",len(random_normal_indices))\n",
    "    print(\"fraud indices in original data\",len(fraud_indices))\n",
    "    print(\"Pruned dataframe shape: \",np.shape(under_sample_data)) \n",
    "    print(\"fraud count in undersample data\",np.count_nonzero(under_sample_data['loan_status']=='Default'))\n",
    "    print(\"Normal count in undersample data\",np.count_nonzero(under_sample_data['loan_status']=='Fully Paid'))\n",
    "    \n",
    "    \n",
    "    return under_sample_data, whole_data\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def Data_Preprocessing(under_sample_data):\n",
    "    \"\"\"\n",
    "    This function does categorization and transformation on balanced classes dataset\n",
    "    input - Balanced classes dataset\n",
    "    output - Balanced classes dataset after modifying its attributes\n",
    "    \"\"\"\n",
    "    dataframe = under_sample_data.copy()\n",
    "    print(\"Cateogrizing interest rate ...\")    \n",
    "    dataframe['interest_rate']='0'\n",
    "    #dataframe.loc[dataframe['int_rate']< 6,'interest_rate']=\"Below6\"\n",
    "    dataframe.loc[(dataframe['int_rate']> 6) & (dataframe['int_rate']<= 10),'interest_rate']=\"6To10\"\n",
    "    dataframe.loc[(dataframe['int_rate']> 10) & (dataframe['int_rate']<= 15),'interest_rate']=\"10To15\"\n",
    "    dataframe.loc[(dataframe['int_rate']> 15) & (dataframe['int_rate']<= 20),'interest_rate']=\"15To20\"\n",
    "    dataframe.loc[(dataframe['int_rate']> 20) & (dataframe['int_rate']<= 25),'interest_rate']=\"20To25\"\n",
    "    dataframe.loc[(dataframe['int_rate']> 25) & (dataframe['int_rate']<= 30),'interest_rate']=\"25To30\"\n",
    "    dataframe.drop(['int_rate'],axis=1, inplace=True)\n",
    "\n",
    "    print(\"Cateogrizing annual_income ...\")  \n",
    "    dataframe['annual_income']='0'\n",
    "    dataframe.loc[(dataframe['annual_inc']> 0) & (dataframe['annual_inc']<= 30000),'annual_income']=\"0To30000\"\n",
    "    dataframe.loc[(dataframe['annual_inc']> 30000) & (dataframe['annual_inc']<= 60000),'annual_income']=\"30000To60000\"\n",
    "    dataframe.loc[(dataframe['annual_inc']> 60000) & (dataframe['annual_inc']<= 90000),'annual_income']=\"60000To90000\"\n",
    "    dataframe.loc[(dataframe['annual_inc']> 90000) & (dataframe['annual_inc']<= 100000),'annual_income']=\"90000To100000\"\n",
    "    dataframe.loc[(dataframe['annual_inc']> 100000) & (dataframe['annual_inc']<= 300000),'annual_income']=\"100000To300000\"\n",
    "    dataframe.loc[(dataframe['annual_inc']> 300000) & (dataframe['annual_inc']<= 600000),'annual_income']=\"300000To600000\"\n",
    "    dataframe.loc[(dataframe['annual_inc']> 600000) ,'annual_income']=\"GreaterThan600000\"\n",
    "    dataframe.drop(['annual_inc'],axis=1, inplace=True)\n",
    "    print (dataframe.columns.tolist())\n",
    "    print (dataframe.head(5))\n",
    "    \n",
    "\n",
    "    print(\"Cateogrizing openAccount ...\") \n",
    "    dataframe['openAccount']='0'\n",
    "    dataframe.loc[(dataframe['open_acc']> 0) & (dataframe['open_acc']<= 10),'openAccount']=\"0To10\"\n",
    "    dataframe.loc[(dataframe['open_acc']> 10) & (dataframe['open_acc']<= 20),'openAccount']=\"10To20\"\n",
    "    dataframe.loc[(dataframe['open_acc']> 20) & (dataframe['open_acc']<= 30),'openAccount']=\"20To30\"\n",
    "    dataframe.loc[(dataframe['open_acc']> 30) & (dataframe['open_acc']<= 40),'openAccount']=\"30To40\"\n",
    "    dataframe.loc[(dataframe['open_acc']> 40) & (dataframe['open_acc']<= 50),'openAccount']=\"40To50\"\n",
    "    dataframe.loc[(dataframe['open_acc']> 50) & (dataframe['open_acc']<= 60),'openAccount']=\"50To60\"\n",
    "    dataframe.loc[(dataframe['open_acc']> 60) & (dataframe['open_acc']<= 70),'openAccount']=\"60To70\"\n",
    "    dataframe.loc[(dataframe['open_acc']> 70) ,'openAccount']=\"GreaterThan70\"\n",
    "    dataframe.drop(['open_acc'],axis=1, inplace=True)\n",
    "    print (dataframe.columns.tolist())\n",
    "    print (dataframe.head(5))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Cateogrizing derog_public_record ...\") \n",
    "    dataframe['derog_public_record']='0'\n",
    "    dataframe.loc[(dataframe['pub_rec']> 0) & (dataframe['pub_rec']<= 5),'derog_public_record']=\"0To5\"\n",
    "    dataframe.loc[(dataframe['pub_rec']> 5) & (dataframe['pub_rec']<= 10),'derog_public_record']=\"5To10\"\n",
    "    dataframe.loc[(dataframe['pub_rec']> 10) & (dataframe['pub_rec']<= 15),'derog_public_record']=\"10To15\"\n",
    "    dataframe.loc[(dataframe['pub_rec']> 15) & (dataframe['pub_rec']<= 20),'derog_public_record']=\"15To20\"\n",
    "    dataframe.loc[(dataframe['pub_rec']> 20) & (dataframe['pub_rec']<= 25),'derog_public_record']=\"20To25\"\n",
    "    dataframe.loc[(dataframe['pub_rec']> 25) & (dataframe['pub_rec']<= 30),'derog_public_record']=\"25To30\"\n",
    "    dataframe.loc[(dataframe['pub_rec']> 30) ,'derog_public_record']=\"GreaterThan70\"\n",
    "    dataframe.drop(['pub_rec'],axis=1, inplace=True)\n",
    "    print (dataframe.columns.tolist())\n",
    "    print (dataframe.head(5))\n",
    "\n",
    "\n",
    "    print(\"Cateogrizing revolving_balance ...\")\n",
    "    dataframe['revolving_balance']='0'\n",
    "    dataframe.loc[(dataframe['revol_bal']> 0) & (dataframe['revol_bal']<= 1000),'revolving_balance']=\"0To1000\"\n",
    "    dataframe.loc[(dataframe['revol_bal']> 1000) & (dataframe['revol_bal']<= 2000),'revolving_balance']=\"1000To2000\"\n",
    "    dataframe.loc[(dataframe['revol_bal']> 2000) & (dataframe['revol_bal']<= 3000),'revolving_balance']=\"2000To3000\"\n",
    "    dataframe.loc[(dataframe['revol_bal']> 3000) & (dataframe['revol_bal']<= 4000),'revolving_balance']=\"3000To4000\"\n",
    "    dataframe.loc[(dataframe['revol_bal']> 4000) & (dataframe['revol_bal']<= 5000),'revolving_balance']=\"4000To5000\"\n",
    "    dataframe.loc[(dataframe['revol_bal']> 5000) & (dataframe['revol_bal']<= 6000),'revolving_balance']=\"5000To6000\"\n",
    "    dataframe.loc[(dataframe['revol_bal']> 6000) & (dataframe['revol_bal']<= 7000),'revolving_balance']=\"6000To7000\"\n",
    "    dataframe.loc[(dataframe['revol_bal']> 7000) & (dataframe['revol_bal']<= 8000),'revolving_balance']=\"7000To8000\"\n",
    "    dataframe.loc[(dataframe['revol_bal']> 8000) & (dataframe['revol_bal']<= 9000),'revolving_balance']=\"8000To9000\"\n",
    "    dataframe.loc[(dataframe['revol_bal']> 9000) ,'revolving_balance']=\"GreaterThan9000\"\n",
    "    dataframe.drop(['revol_bal'],axis=1, inplace=True)\n",
    "    print (dataframe.columns.tolist())\n",
    "    print (dataframe.head(5))\n",
    "\n",
    "\n",
    "    print(\"Cateogrizing funded_amount ...\")\n",
    "    dataframe['funded_amount']='0'\n",
    "    dataframe.loc[(dataframe['funded_amnt']> 0) & (dataframe['funded_amnt']<= 5000),'funded_amount']=\"0To5000\"\n",
    "    dataframe.loc[(dataframe['funded_amnt']> 5000) & (dataframe['funded_amnt']<= 10000),'funded_amount']=\"5000To10000\"\n",
    "    dataframe.loc[(dataframe['funded_amnt']> 10000) & (dataframe['funded_amnt']<= 15000),'funded_amount']=\"10000To15000\"\n",
    "    dataframe.loc[(dataframe['funded_amnt']> 15000) & (dataframe['funded_amnt']<= 20000),'funded_amount']=\"15000To20000\"\n",
    "    dataframe.loc[(dataframe['funded_amnt']> 20000) & (dataframe['funded_amnt']<= 25000),'funded_amount']=\"20000To25000\"\n",
    "    dataframe.loc[(dataframe['funded_amnt']> 25000) & (dataframe['funded_amnt']<= 30000),'funded_amount']=\"25000To30000\"\n",
    "    dataframe.loc[dataframe['funded_amnt']> 30000, 'funded_amount']=\"GreaterThan30000\"\n",
    "    dataframe.drop(['funded_amnt'],axis=1, inplace=True)\n",
    "    dataframe['emp_length'].fillna('0',inplace=True)\n",
    "    dataframe['emp_length'].replace(to_replace=' years', value='', regex=True, inplace=True)\n",
    "    dataframe['emp_length'].replace(to_replace='\\+ years', value='', regex=True, inplace=True)\n",
    "    dataframe['emp_length'].replace(to_replace='< 1 year', value='0', regex=True, inplace=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def RoundOff_Data(dataframe):\n",
    "    \"\"\"\n",
    "    This function does rounding off of columns having nearly same decimal values\n",
    "    input - balanced classes dataset\n",
    "    output - balanced classes dataset with some columns rounded off\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\\nRounding Off...\\n\")    \n",
    "    temp_dataframe = pd.DataFrame()\n",
    "    cols_ToRoundOff=['total_rec_late_fee', 'total_pymnt_inv', 'total_pymnt','funded_amnt_inv','last_pymnt_amnt' ]\n",
    "    \n",
    "    \n",
    "    for colname in cols_ToRoundOff:\n",
    "        temp_dataframe[colname] = dataframe[colname].round()\n",
    "        dataframe.drop([colname],axis=1, inplace=True)\n",
    "\n",
    "    dataset = pd.concat([dataframe, temp_dataframe], axis=1)\n",
    "    print(np.shape(dataset))\n",
    "    \n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Encoding(dataset):\n",
    "    \"\"\"\n",
    "    This function does label encoding & One-Hot-Encoding of all dataset and converts it into encoded dataframe\n",
    "    input - dataset with balanced classes and mixed attributes\n",
    "    output - dataset with balanced classes in encoded form as numeric\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Label Encoding ... \")\n",
    "    Path(Tosave_Folder+\"/\"+'encoding').mkdir( exist_ok=True) \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Label encoding for each feature\n",
    "    leX = LabelEncoder()\n",
    "    for colName in dataset.columns.tolist():\n",
    "            dataset[colName] = leX.fit_transform((dataset[colName].astype(str)))\n",
    "            np.save(Tosave_Folder+\"/encoding/\"+colName+\".npy\",leX.classes_)\n",
    "            #y_LabEnc= dataset['Default'].copy()\n",
    "\n",
    "    #Separating target class\n",
    "    y_labEnc= dataset['loan_status'].copy()\n",
    "    dataset.drop(['loan_status'],inplace=True, axis=1)\n",
    "    \n",
    "    #Separate feature matrix\n",
    "    X_labEnc = dataset.iloc[:,:].copy()\n",
    "    \n",
    "    # One-Hot-Encoding\n",
    "    ohe_X = OneHotEncoder()\n",
    "    X_OHE = ohe_X.fit_transform(X_labEnc)\n",
    "    print(\"\\nOne Hot Encoded Data\",np.shape(X_OHE))\n",
    "    \n",
    "    #need first 2 for neural networks & X_labEnc for XGBS\n",
    "    return X_OHE, y_labEnc, X_labEnc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Rescaling(X_Train, X_Test):\n",
    "  \"\"\"\n",
    "  This function does feature scaling for Training set and Testing set\n",
    "  input - Spllitted and one hot encoded Training set and Test set\n",
    "  output - Rescaled Training and Test set\n",
    "  \"\"\"\n",
    "  norm = MaxAbsScaler()\n",
    "  X_Train = norm.fit_transform(X_Train)\n",
    "  X_Test  = norm.transform(X_Test)\n",
    "  return X_Train, X_Test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Inv_Enc(y_P, y_Te):\n",
    "    \"\"\"\n",
    "    This function returns the actual values of the target classes after the prediction is done\n",
    "    on the Test set.\n",
    "    Input - Encoded True values and Encoded Predicted values of the Target Class\n",
    "    Output- De-Encoded True values and De-Encoded predicted values of the Target Class.\n",
    "    \"\"\"\n",
    "\n",
    "    Le_Decode= LabelEncoder()\n",
    "    fileToRead = Tosave_Folder+\"/encoding/\"+str(targetName)+\".npy\"\n",
    "    if is_File(fileToRead) :\n",
    "            Le_Decode.classes_ = np.load(fileToRead)\n",
    "            le_classes = Le_Decode.classes_.tolist()\n",
    "            Le_Decode.classes = le_classes\n",
    "\n",
    "            y_P_Decoded = Le_Decode.inverse_transform(y_P)\n",
    "            y_T_Decoded = Le_Decode.inverse_transform(y_Te)\n",
    "            \n",
    "            return y_P_Decoded, y_T_Decoded\n",
    "    else:\n",
    "        print(\"numpy file for Target Class Not Found !!\")\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def Performace_Metrics(y_Pred, y_Test, y_Train, classifier_fold_info):\n",
    "        \"\"\"\n",
    "        This function computes the classification metrics on the balanced classes dataset\n",
    "        and writes the metrics for each fold into the Output.txt file.\n",
    "        These metrics mainly incude Accuracy, Precision, Recall, F1-Score.\n",
    "        Input - Label Encoded & OneHotEncoded (Predictes values of Test data, True values of Test data,\n",
    "                                               True values of Training data), dictionary having classifer info \n",
    "                                               to be dumped in output.txt file\n",
    "        Output - None\n",
    "        \"\"\"\n",
    "        # decode labels\n",
    "        y_Train_Decoded, y_Test_Decoded = Inv_Enc(y_Train, y_Test)\n",
    "        y_Pred_Decoded, y_Test_Decoded = Inv_Enc(y_Pred, y_Test)    \n",
    "        \n",
    "        # plot confusion matrix\n",
    "        cnf_matrix = confusion_matrix(y_Test_Decoded, y_Pred_Decoded)\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cnf_matrix, classes=[\"Default\",\"Fully Paid\"], title='Confusion matrix')\n",
    "        plt.savefig(Tosave_Folder+\"/\"+classifier_fold_info['classifier']+\"_\"+str(classifier_fold_info['splitCount'])+\"ConfusionMatx\")\n",
    "        plt.close()\n",
    "        \n",
    "        # calculate performance metric\n",
    "        correctTotal=0\n",
    "        for true,pred in zip(y_Test, y_Pred):\n",
    "            if true == pred:\n",
    "                correctTotal = correctTotal +1\n",
    "        acc = np.round((float (correctTotal)/int(len(y_Test)))*100,2)\n",
    "        classifier_fold_info['Accuracy']=str(acc)\n",
    "        classifier_fold_info[\"metrics\"] = classification_report(y_Test, y_Pred,target_names=[\"Default\",\"Fully Paid\"])\n",
    "        with open(Tosave_Folder+\"/Output.json\",\"a\") as fw:\n",
    "            json.dump(classifier_fold_info, fw,indent=2)\n",
    "        print(\"\\n\\nAccuracy: \",acc)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    \n",
    "    \n",
    "def Create_NN(X_Train, X_TrainTest, y_Train, y_TrainTest, X_Test, y_Test, classifier_fold_info):\n",
    "    \"\"\"\n",
    "    This function will creat Neural Network model utilizing keras sequential model with Tensorflow backend.\n",
    "    It will also train model on Training data and validate it on Validation data.\n",
    "    It also plots the learning behaviour of the algorithm by plotting accuracy achieved on y-axis and\n",
    "    epochs run on x-axis.\n",
    "    Input - Splitted Training set, Validation set, Test set, dictionary_storing_classifier_info\n",
    "    Output- predicted classes for Test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #------------------------\n",
    "    # Define neural network\n",
    "    #-----------------------\n",
    "    model = Sequential()\n",
    "    optimizer = Adam(lr=0.01)\n",
    "    model.add(Dense(200,activation ='relu',input_dim=np.shape(X_Train)[1]))\n",
    "#   model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1, activation = \"sigmoid\"))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    callback=[EarlyStopping(monitor='val_loss',patience=5, verbose=1),\n",
    "              ModelCheckpoint(Tosave_Folder+\"/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\", monitor='val_acc', \n",
    "                              verbose=1, save_best_only=True,mode='auto',save_weights_only=False)]\n",
    "              \n",
    "              \n",
    "    print(\"\\nTraining Data shape is:\",np.shape(X_Train))\n",
    "    \n",
    "\n",
    "    start_time= datetime.now()\n",
    "    history=model.fit(X_Train, y_Train, validation_data=(X_TrainTest,y_TrainTest), epochs=10, callbacks=callback,\n",
    "                      batch_size = 32)\n",
    "\n",
    "    classifier_fold_info[\"Time taken\"] = str(datetime.now()-start_time)    \n",
    "\n",
    "\n",
    "\n",
    "    # -- Plot\n",
    "    # summarize history for accuracy\n",
    "    plt.figure()\n",
    "    plt.plot( history.history['acc'])\n",
    "    plt.plot( history.history['val_acc'])\n",
    "    plt.title(\"NN_Fold\"+str(classifier_fold_info['splitCount'])+\"Training_Validation_Accuracy\")\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.legend(['train', 'test'], loc='lower left')\n",
    "    plt.savefig(Tosave_Folder+\"/Fold\"+str(classifier_fold_info['splitCount'])+\"TrainAccuracy\")\n",
    "    plt.close()\n",
    "    # summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot( history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(\"NN_Fold\"+str(classifier_fold_info['splitCount'])+\"TrainLoss\")\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(Tosave_Folder+\"/Fold\"+str(classifier_fold_info['splitCount'])+\"TrainLoss\")\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    # do predictions\n",
    "    y_Pred =model.predict_classes(X_Test)\n",
    "    \n",
    "    return y_Pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def Neural_Network(X_OHE,y_labEnc):    \n",
    "    \"\"\"\n",
    "    This function implements the neural network algorithm. It does the stratified splitting of encoded data\n",
    "    into Training, Validation and Test sets. In order to do cross-validation this function repeats the process\n",
    "    of splitting 4 times and store the fold count in the dictionary. After splitting, it calls rescaling function\n",
    "    to scale the features and then calls Create_NN for model creating, training and prediction on splitted dataset.\n",
    "    Input - One-Hot-Encoded input sparse Matrix, Label Encoded Target Class\n",
    "    Output - Computes the average of 4 fold cross-validation of neural network performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    #-----------------------------------------------------------------\n",
    "    #           Neural Network \n",
    "    #-----------------------------------------------------------------\n",
    "    \n",
    "    classifier_fold_info ={}\n",
    "    classifier_fold_info['splitCount']=1\n",
    "    classifier_fold_info['classifier']=\"NeuralNetwork\"\n",
    "\n",
    "    # Stratified classes will generate balanced classes.\n",
    "    kf = StratifiedKFold(n_splits=4, random_state=seed)\n",
    "\n",
    "    for train, test in kf.split(X_OHE,y_labEnc):\n",
    "        # X = np.array(X_OHE)\n",
    "        y_labEnc = np.array(y_labEnc)\n",
    "        X_Train, X_Test, y_Train, y_Test = X_OHE[train], X_OHE[test], y_labEnc[train], y_labEnc[test]\n",
    "        \n",
    "        X_Train, X_Test = Rescaling(X_Train, X_Test)\n",
    "        X_Train, X_TrainTest, y_Train, y_TrainTest = train_test_split(X_Train,y_Train, test_size=0.33, random_state=seed)\n",
    "        \n",
    "        # --Model--\n",
    "        y_Pred = Create_NN(X_Train, X_TrainTest, y_Train, y_TrainTest, X_Test, y_Test, classifier_fold_info)\n",
    "        \n",
    "        Performace_Metrics(y_Pred, y_Test, y_Train, classifier_fold_info)\n",
    "        classifier_fold_info['splitCount']=classifier_fold_info['splitCount']+1\n",
    "    \n",
    "\n",
    "    \n",
    "    # Result    \n",
    "    average_accuracy =float(np.sum(accuracies[-4:]) /4)\n",
    "    print(\"\\n\\nNeural Network  4-fold cross validation\", average_accuracy)\n",
    "\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "def Xgboost(X_labEnc,y_labEnc):\n",
    "    \"\"\"\n",
    "    This function implements the Xgboost algorithm. It does the stratified splitting of encoded data\n",
    "    into Training, Validation and Test sets. In order to do cross-validation this function repeats the process\n",
    "    of splitting 4 times and store the fold count in the dictionary. After splitting, to scale the features \n",
    "    and then trains the model on training data and prediction on splitted dataset.\n",
    "    Input - Label Encoded Input array, Label Encoded Target Class\n",
    "    Output - Computes the average of 4 fold cross-validation of Xgboost performance.\n",
    "    \"\"\"    \n",
    "\n",
    "    classifier_fold_info ={}\n",
    "    classifier_fold_info['splitCount']=1\n",
    "    classifier_fold_info['classifier']=\"Xgboost_Stratified\"\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    #           XGBoost  - stratified sampling\n",
    "    #------------------------------------------------\n",
    "    kf = StratifiedKFold(n_splits=4, random_state=seed)\n",
    "    for train, test in kf.split(X_labEnc,y_labEnc):\n",
    "\n",
    "        X = np.array(X_labEnc)\n",
    "        y = np.array(y_labEnc)\n",
    "\n",
    "        # split train,test\n",
    "        X_Train, X_Test, y_Train, y_Test = X[train], X[test], y[train], y[test]\n",
    "        X_Train, X_TrainTest, y_Train, y_TrainTest = train_test_split(X_Train,y_Train, test_size=0.33, random_state=1)\n",
    "      \n",
    "        # define model\n",
    "        clf = xgboost.sklearn.XGBClassifier(objective=\"binary:logistic\",learning_rate=0.01,seed=1,\n",
    "                                            max_depth=20, gamma=10, n_estimators=300)\n",
    "        # fit model\n",
    "        start_time= datetime.now()        \n",
    "        clf.fit(X_Train, y_Train, early_stopping_rounds=5,eval_set=[(X_TrainTest,y_TrainTest)], eval_metric=\"auc\", verbose=True)\n",
    "        classifier_fold_info[\"Time taken\"] = str(datetime.now()-start_time)    \n",
    "\n",
    "        y_Pred = clf.predict(X_Test)        \n",
    "        predictions = [round(value) for value in y_Pred]\n",
    "        y_Pred = predictions\n",
    "        \n",
    "        # Decode Labels in predictions\n",
    "        Performace_Metrics(y_Pred, y_Test, y_Train, classifier_fold_info)\n",
    "        classifier_fold_info['splitCount']=classifier_fold_info['splitCount']+1\n",
    "    \n",
    "    # result    \n",
    "    average_accuracy =float(np.sum(accuracies[-4:]) /4)\n",
    "    print(\"\\n\\n XGBoost 4-fold cross validation\", average_accuracy)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def Xgboost_Unstratified(X_labEnc,y_labEnc): \n",
    "    \"\"\"\n",
    "    This function implements the Xgboost algorithm. It does the un-stratified splitting of encoded data\n",
    "    into Training, Validation and Test sets. In order to do cross-validation this function repeats the process\n",
    "    of splitting 4 times and store the fold count in the dictionary. After splitting, to scale the features \n",
    "    and then trains the model on training data and prediction on splitted dataset.\n",
    "    Input - Label Encoded Input array, Label Encoded Target Class\n",
    "    Output - Computes the average of 4 fold cross-validation of Xgboost performance.\n",
    "    \"\"\"     \n",
    "\n",
    "    classifier_fold_info ={}\n",
    "    classifier_fold_info['splitCount']=1\n",
    "    classifier_fold_info['classifier']=\"Xgboost_Unstratified\"\n",
    "    #------------------------------------------------\n",
    "    #           XGBoost  -  non-stratified sampling\n",
    "    #------------------------------------------------\n",
    "    for splits in range(4):\n",
    "\n",
    "        X_Train, X_Test, y_Train, y_Test = train_test_split(X_labEnc,y_labEnc,test_size=0.33)\n",
    "        X_Train, X_TrainTest, y_Train, y_TrainTest = train_test_split(X_Train,y_Train, test_size=0.33, \n",
    "                                                                      random_state=seed)\n",
    "      \n",
    "        # define model\n",
    "        clf = xgboost.sklearn.XGBClassifier(objective=\"binary:logistic\",learning_rate=0.01,seed=seed,\n",
    "                                            max_depth=20, gamma=10, n_estimators=300)\n",
    "        # fit model\n",
    "        start_time= datetime.now()  \n",
    "        clf.fit(X_Train, y_Train, early_stopping_rounds=5,eval_set=[(X_TrainTest,y_TrainTest)], eval_metric=\"auc\", \n",
    "                                                                    verbose=True)\n",
    "        classifier_fold_info[\"Time taken\"] = str(datetime.now()-start_time)    \n",
    "\n",
    "            \n",
    "        y_Pred = clf.predict(X_Test)        \n",
    "        predictions = [round(value) for value in y_Pred]\n",
    "        y_Pred = predictions\n",
    "        \n",
    "        # Decode Labels in predictions\n",
    "        Performace_Metrics(y_Pred, y_Test, y_Train, classifier_fold_info)\n",
    "        classifier_fold_info['splitCount']=classifier_fold_info['splitCount']+1\n",
    "    \n",
    "    # result    \n",
    "    print(accuracies)\n",
    "    average_accuracy =(np.sum(accuracies[-4:])/4)\n",
    "    print(\"\\n\\nXGBoost unstratified 4-fold cross validation\", average_accuracy)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Logistic_Regression(X_OHE,y_labEnc):\n",
    "    \"\"\"\n",
    "    This function implements the Logistic Regression algorithm. It does the stratified splitting of encoded data\n",
    "    into Training, Validation and Test sets. In order to do cross-validation this function repeats the process\n",
    "    of splitting 4 times and store the fold count in the dictionary. After splitting, to scale the features \n",
    "    it call rescaling and then trains the model on training data and prediction on splitted dataset.\n",
    "    Input - One-Hot-Encoded Input array, Label Encoded Target Class\n",
    "    Output - Computes the average of 4 fold cross-validation of Logistic Regression performance    \n",
    "    \"\"\"\n",
    "    classifier_fold_info ={}\n",
    "    classifier_fold_info['splitCount']=1\n",
    "    classifier_fold_info['classifier']=\"Logistic Regression\"\n",
    "    logreg = LogisticRegression(penalty='l2', tol=0.0001, random_state=seed,solver='liblinear', max_iter=50,verbose=1)\n",
    "                                # liblinear is a good choice for small datasets\n",
    "\n",
    "\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=4, random_state=seed)\n",
    "\n",
    "    for train, test in kf.split(X_OHE,y_labEnc):\n",
    "        X = np.array(X_OHE)\n",
    "        y_labEnc = np.array(y_labEnc)\n",
    "        X_Train, X_Test, y_Train, y_Test = X_OHE[train], X_OHE[test], y_labEnc[train], y_labEnc[test]\n",
    "        \n",
    "        X_Train, X_Test = Rescaling(X_Train, X_Test)\n",
    "        X_Train, X_TrainTest, y_Train, y_TrainTest = train_test_split(X_Train,y_Train, test_size=0.33, random_state=seed)\n",
    "        start_time= datetime.now()  \n",
    "        \n",
    "\n",
    "        logreg.fit(X_Train, y_Train)\n",
    "        classifier_fold_info[\"Time taken\"] = str(datetime.now()-start_time) \n",
    "\n",
    "        y_Pred = logreg.predict(X_Test)\n",
    "\n",
    "\n",
    "        # Decode Labels in predictions\n",
    "        Performace_Metrics(y_Pred, y_Test, y_Train, classifier_fold_info)\n",
    "        classifier_fold_info['splitCount'] =  classifier_fold_info['splitCount']+1\n",
    "    \n",
    "    # result    \n",
    "    print(accuracies)\n",
    "    average_accuracy =(np.sum(accuracies[:])/4)\n",
    "    print(\"\\n\\nLogistic Regression 4-fold cross validation\", average_accuracy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with filtered columns - shape is:  (887379, 18)\n"
     ]
    }
   ],
   "source": [
    "data_subset =Read_Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       funded_amnt  funded_amnt_inv  int_rate  loan_amnt loan_status  \\\n",
      "318          18000     17975.000000     17.27      18000     Default   \n",
      "7620         21250     21003.604048     14.27      21250     Default   \n",
      "11759         5600      5600.000000     15.99       5600     Default   \n",
      "13439        15975     15975.000000     20.99      15975     Default   \n",
      "13856         5000      5000.000000     15.99       5000     Default   \n",
      "\n",
      "      pymnt_plan        term sub_grade  annual_inc emp_length  open_acc  \\\n",
      "318            n   60 months        D3     62000.0     1 year       8.0   \n",
      "7620           n   60 months        C2     36000.0    5 years      20.0   \n",
      "11759          n   60 months        D2     52416.0    8 years       8.0   \n",
      "13439          n   60 months        E5    225000.0    8 years      10.0   \n",
      "13856          n   60 months        D2     65004.0  10+ years      15.0   \n",
      "\n",
      "       pub_rec  total_acc  last_pymnt_amnt  revol_bal  total_pymnt  \\\n",
      "318        0.0       30.0           449.97    11273.0     19767.48   \n",
      "7620       0.0       25.0           497.43    16753.0     23343.83   \n",
      "11759      0.0       13.0           136.16     1258.0      6643.32   \n",
      "13439      0.0       27.0           432.09    10852.0     21550.10   \n",
      "13856      1.0       33.0           121.57     2274.0      6048.10   \n",
      "\n",
      "       total_pymnt_inv  total_rec_late_fee  \n",
      "318           19740.11                 0.0  \n",
      "7620          22895.16                 0.0  \n",
      "11759          6643.32                 0.0  \n",
      "13439         21550.10                 0.0  \n",
      "13856          6048.10                 0.0  \n",
      "Normal indices 1219\n",
      "fraud indices in original data 1219\n",
      "Pruned dataframe shape:  (2438, 18)\n",
      "fraud count in undersample data 1219\n",
      "Normal count in undersample data 1219\n"
     ]
    }
   ],
   "source": [
    "under_sample_data, whole_data = Under_Sampling(data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cateogrizing interest rate ...\n",
      "Cateogrizing annual_income ...\n",
      "['funded_amnt', 'funded_amnt_inv', 'loan_amnt', 'loan_status', 'pymnt_plan', 'term', 'sub_grade', 'emp_length', 'open_acc', 'pub_rec', 'total_acc', 'last_pymnt_amnt', 'revol_bal', 'total_pymnt', 'total_pymnt_inv', 'total_rec_late_fee', 'interest_rate', 'annual_income']\n",
      "       funded_amnt  funded_amnt_inv  loan_amnt loan_status pymnt_plan  \\\n",
      "318          18000     17975.000000      18000     Default          n   \n",
      "7620         21250     21003.604048      21250     Default          n   \n",
      "11759         5600      5600.000000       5600     Default          n   \n",
      "13439        15975     15975.000000      15975     Default          n   \n",
      "13856         5000      5000.000000       5000     Default          n   \n",
      "\n",
      "             term sub_grade emp_length  open_acc  pub_rec  total_acc  \\\n",
      "318     60 months        D3     1 year       8.0      0.0       30.0   \n",
      "7620    60 months        C2    5 years      20.0      0.0       25.0   \n",
      "11759   60 months        D2    8 years       8.0      0.0       13.0   \n",
      "13439   60 months        E5    8 years      10.0      0.0       27.0   \n",
      "13856   60 months        D2  10+ years      15.0      1.0       33.0   \n",
      "\n",
      "       last_pymnt_amnt  revol_bal  total_pymnt  total_pymnt_inv  \\\n",
      "318             449.97    11273.0     19767.48         19740.11   \n",
      "7620            497.43    16753.0     23343.83         22895.16   \n",
      "11759           136.16     1258.0      6643.32          6643.32   \n",
      "13439           432.09    10852.0     21550.10         21550.10   \n",
      "13856           121.57     2274.0      6048.10          6048.10   \n",
      "\n",
      "       total_rec_late_fee interest_rate   annual_income  \n",
      "318                   0.0        15To20    60000To90000  \n",
      "7620                  0.0        10To15    30000To60000  \n",
      "11759                 0.0        15To20    30000To60000  \n",
      "13439                 0.0        20To25  100000To300000  \n",
      "13856                 0.0        15To20    60000To90000  \n",
      "Cateogrizing openAccount ...\n",
      "['funded_amnt', 'funded_amnt_inv', 'loan_amnt', 'loan_status', 'pymnt_plan', 'term', 'sub_grade', 'emp_length', 'pub_rec', 'total_acc', 'last_pymnt_amnt', 'revol_bal', 'total_pymnt', 'total_pymnt_inv', 'total_rec_late_fee', 'interest_rate', 'annual_income', 'openAccount']\n",
      "       funded_amnt  funded_amnt_inv  loan_amnt loan_status pymnt_plan  \\\n",
      "318          18000     17975.000000      18000     Default          n   \n",
      "7620         21250     21003.604048      21250     Default          n   \n",
      "11759         5600      5600.000000       5600     Default          n   \n",
      "13439        15975     15975.000000      15975     Default          n   \n",
      "13856         5000      5000.000000       5000     Default          n   \n",
      "\n",
      "             term sub_grade emp_length  pub_rec  total_acc  last_pymnt_amnt  \\\n",
      "318     60 months        D3     1 year      0.0       30.0           449.97   \n",
      "7620    60 months        C2    5 years      0.0       25.0           497.43   \n",
      "11759   60 months        D2    8 years      0.0       13.0           136.16   \n",
      "13439   60 months        E5    8 years      0.0       27.0           432.09   \n",
      "13856   60 months        D2  10+ years      1.0       33.0           121.57   \n",
      "\n",
      "       revol_bal  total_pymnt  total_pymnt_inv  total_rec_late_fee  \\\n",
      "318      11273.0     19767.48         19740.11                 0.0   \n",
      "7620     16753.0     23343.83         22895.16                 0.0   \n",
      "11759     1258.0      6643.32          6643.32                 0.0   \n",
      "13439    10852.0     21550.10         21550.10                 0.0   \n",
      "13856     2274.0      6048.10          6048.10                 0.0   \n",
      "\n",
      "      interest_rate   annual_income openAccount  \n",
      "318          15To20    60000To90000       0To10  \n",
      "7620         10To15    30000To60000      10To20  \n",
      "11759        15To20    30000To60000       0To10  \n",
      "13439        20To25  100000To300000       0To10  \n",
      "13856        15To20    60000To90000      10To20  \n",
      "Cateogrizing derog_public_record ...\n",
      "['funded_amnt', 'funded_amnt_inv', 'loan_amnt', 'loan_status', 'pymnt_plan', 'term', 'sub_grade', 'emp_length', 'total_acc', 'last_pymnt_amnt', 'revol_bal', 'total_pymnt', 'total_pymnt_inv', 'total_rec_late_fee', 'interest_rate', 'annual_income', 'openAccount', 'derog_public_record']\n",
      "       funded_amnt  funded_amnt_inv  loan_amnt loan_status pymnt_plan  \\\n",
      "318          18000     17975.000000      18000     Default          n   \n",
      "7620         21250     21003.604048      21250     Default          n   \n",
      "11759         5600      5600.000000       5600     Default          n   \n",
      "13439        15975     15975.000000      15975     Default          n   \n",
      "13856         5000      5000.000000       5000     Default          n   \n",
      "\n",
      "             term sub_grade emp_length  total_acc  last_pymnt_amnt  revol_bal  \\\n",
      "318     60 months        D3     1 year       30.0           449.97    11273.0   \n",
      "7620    60 months        C2    5 years       25.0           497.43    16753.0   \n",
      "11759   60 months        D2    8 years       13.0           136.16     1258.0   \n",
      "13439   60 months        E5    8 years       27.0           432.09    10852.0   \n",
      "13856   60 months        D2  10+ years       33.0           121.57     2274.0   \n",
      "\n",
      "       total_pymnt  total_pymnt_inv  total_rec_late_fee interest_rate  \\\n",
      "318       19767.48         19740.11                 0.0        15To20   \n",
      "7620      23343.83         22895.16                 0.0        10To15   \n",
      "11759      6643.32          6643.32                 0.0        15To20   \n",
      "13439     21550.10         21550.10                 0.0        20To25   \n",
      "13856      6048.10          6048.10                 0.0        15To20   \n",
      "\n",
      "        annual_income openAccount derog_public_record  \n",
      "318      60000To90000       0To10                   0  \n",
      "7620     30000To60000      10To20                   0  \n",
      "11759    30000To60000       0To10                   0  \n",
      "13439  100000To300000       0To10                   0  \n",
      "13856    60000To90000      10To20                0To5  \n",
      "Cateogrizing revolving_balance ...\n",
      "['funded_amnt', 'funded_amnt_inv', 'loan_amnt', 'loan_status', 'pymnt_plan', 'term', 'sub_grade', 'emp_length', 'total_acc', 'last_pymnt_amnt', 'total_pymnt', 'total_pymnt_inv', 'total_rec_late_fee', 'interest_rate', 'annual_income', 'openAccount', 'derog_public_record', 'revolving_balance']\n",
      "       funded_amnt  funded_amnt_inv  loan_amnt loan_status pymnt_plan  \\\n",
      "318          18000     17975.000000      18000     Default          n   \n",
      "7620         21250     21003.604048      21250     Default          n   \n",
      "11759         5600      5600.000000       5600     Default          n   \n",
      "13439        15975     15975.000000      15975     Default          n   \n",
      "13856         5000      5000.000000       5000     Default          n   \n",
      "\n",
      "             term sub_grade emp_length  total_acc  last_pymnt_amnt  \\\n",
      "318     60 months        D3     1 year       30.0           449.97   \n",
      "7620    60 months        C2    5 years       25.0           497.43   \n",
      "11759   60 months        D2    8 years       13.0           136.16   \n",
      "13439   60 months        E5    8 years       27.0           432.09   \n",
      "13856   60 months        D2  10+ years       33.0           121.57   \n",
      "\n",
      "       total_pymnt  total_pymnt_inv  total_rec_late_fee interest_rate  \\\n",
      "318       19767.48         19740.11                 0.0        15To20   \n",
      "7620      23343.83         22895.16                 0.0        10To15   \n",
      "11759      6643.32          6643.32                 0.0        15To20   \n",
      "13439     21550.10         21550.10                 0.0        20To25   \n",
      "13856      6048.10          6048.10                 0.0        15To20   \n",
      "\n",
      "        annual_income openAccount derog_public_record revolving_balance  \n",
      "318      60000To90000       0To10                   0   GreaterThan9000  \n",
      "7620     30000To60000      10To20                   0   GreaterThan9000  \n",
      "11759    30000To60000       0To10                   0        1000To2000  \n",
      "13439  100000To300000       0To10                   0   GreaterThan9000  \n",
      "13856    60000To90000      10To20                0To5        2000To3000  \n",
      "Cateogrizing funded_amount ...\n"
     ]
    }
   ],
   "source": [
    "dataframe = Data_Preprocessing(under_sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rounding Off...\n",
      "\n",
      "(2438, 18)\n"
     ]
    }
   ],
   "source": [
    "dataset = RoundOff_Data(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding ... \n",
      "\n",
      "One Hot Encoded Data (2438, 7514)\n"
     ]
    }
   ],
   "source": [
    "X_OHE, y_labEnc, X_labEnc = Encoding(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][[197 108]\n",
      " [107 198]]\n",
      "\n",
      "\n",
      "Accuracy:  64.75\n",
      "[LibLinear][[218  87]\n",
      " [109 196]]\n",
      "\n",
      "\n",
      "Accuracy:  67.87\n",
      "[LibLinear][[191 114]\n",
      " [107 198]]\n",
      "\n",
      "\n",
      "Accuracy:  63.77\n",
      "[LibLinear][[210  94]\n",
      " [103 201]]\n",
      "\n",
      "\n",
      "Accuracy:  67.6\n",
      "[64.75, 67.870000000000005, 63.770000000000003, 67.599999999999994]\n",
      "\n",
      "\n",
      "Logistic Regression 4-fold cross validation 65.9975\n"
     ]
    }
   ],
   "source": [
    "Logistic_Regression(X_OHE,y_labEnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Data shape is: (1224, 7514)\n",
      "Train on 1224 samples, validate on 604 samples\n",
      "Epoch 1/10\n",
      "1224/1224 [==============================] - 2s 2ms/step - loss: 0.6622 - acc: 0.5874 - val_loss: 0.6111 - val_acc: 0.6556\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.65563, saving model to Credit_Defaulter/weights-improvement-01-0.66.hdf5\n",
      "Epoch 2/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 0.3346 - acc: 0.8587 - val_loss: 0.6320 - val_acc: 0.6887\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.65563 to 0.68874, saving model to Credit_Defaulter/weights-improvement-02-0.69.hdf5\n",
      "Epoch 3/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 0.0507 - acc: 0.9935 - val_loss: 0.7021 - val_acc: 0.6887\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.68874\n",
      "Epoch 4/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.7383 - val_acc: 0.7020\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.68874 to 0.70199, saving model to Credit_Defaulter/weights-improvement-04-0.70.hdf5\n",
      "Epoch 5/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.7854 - val_acc: 0.6921\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.70199\n",
      "Epoch 6/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 7.0290e-04 - acc: 1.0000 - val_loss: 0.7947 - val_acc: 0.6854\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.70199\n",
      "Epoch 00006: early stopping\n",
      "[[197 108]\n",
      " [121 184]]\n",
      "\n",
      "\n",
      "Accuracy:  62.46\n",
      "\n",
      "Training Data shape is: (1224, 7514)\n",
      "Train on 1224 samples, validate on 604 samples\n",
      "Epoch 1/10\n",
      "1224/1224 [==============================] - 2s 2ms/step - loss: 0.6605 - acc: 0.6152 - val_loss: 0.6500 - val_acc: 0.6507\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.65066, saving model to Credit_Defaulter/weights-improvement-01-0.65.hdf5\n",
      "Epoch 2/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 0.3084 - acc: 0.8701 - val_loss: 0.7360 - val_acc: 0.6159\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.65066\n",
      "Epoch 3/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 0.0444 - acc: 0.9926 - val_loss: 0.8924 - val_acc: 0.6159\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.65066\n",
      "Epoch 4/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 0.0047 - acc: 0.9992 - val_loss: 0.8724 - val_acc: 0.6656\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.65066 to 0.66556, saving model to Credit_Defaulter/weights-improvement-04-0.67.hdf5\n",
      "Epoch 5/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 8.1979e-04 - acc: 1.0000 - val_loss: 0.8953 - val_acc: 0.6639\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.66556\n",
      "Epoch 6/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 4.1473e-04 - acc: 1.0000 - val_loss: 0.8993 - val_acc: 0.6606\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.66556\n",
      "Epoch 00006: early stopping\n",
      "[[212  93]\n",
      " [119 186]]\n",
      "\n",
      "\n",
      "Accuracy:  65.25\n",
      "\n",
      "Training Data shape is: (1224, 7514)\n",
      "Train on 1224 samples, validate on 604 samples\n",
      "Epoch 1/10\n",
      "1224/1224 [==============================] - 2s 2ms/step - loss: 0.6448 - acc: 0.6324 - val_loss: 0.6053 - val_acc: 0.6821\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.68212, saving model to Credit_Defaulter/weights-improvement-01-0.68.hdf5\n",
      "Epoch 2/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 0.2553 - acc: 0.9093 - val_loss: 0.6441 - val_acc: 0.7053\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.68212 to 0.70530, saving model to Credit_Defaulter/weights-improvement-02-0.71.hdf5\n",
      "Epoch 3/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 0.0239 - acc: 0.9959 - val_loss: 0.6908 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.70530 to 0.71358, saving model to Credit_Defaulter/weights-improvement-03-0.71.hdf5\n",
      "Epoch 4/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.7720 - val_acc: 0.6904\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.71358\n",
      "Epoch 5/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 5.3014e-04 - acc: 1.0000 - val_loss: 0.7338 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.71358 to 0.72682, saving model to Credit_Defaulter/weights-improvement-05-0.73.hdf5\n",
      "Epoch 6/10\n",
      "1224/1224 [==============================] - 2s 1ms/step - loss: 3.0999e-04 - acc: 1.0000 - val_loss: 0.7445 - val_acc: 0.7235\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.72682\n",
      "Epoch 00006: early stopping\n",
      "[[188 117]\n",
      " [113 192]]\n",
      "\n",
      "\n",
      "Accuracy:  62.3\n",
      "\n",
      "Training Data shape is: (1226, 7514)\n",
      "Train on 1226 samples, validate on 604 samples\n",
      "Epoch 1/10\n",
      "1226/1226 [==============================] - 2s 2ms/step - loss: 0.6573 - acc: 0.6305 - val_loss: 0.6324 - val_acc: 0.6358\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63576, saving model to Credit_Defaulter/weights-improvement-01-0.64.hdf5\n",
      "Epoch 2/10\n",
      "1226/1226 [==============================] - 2s 1ms/step - loss: 0.3018 - acc: 0.8850 - val_loss: 0.7048 - val_acc: 0.6457\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.63576 to 0.64570, saving model to Credit_Defaulter/weights-improvement-02-0.65.hdf5\n",
      "Epoch 3/10\n",
      "1226/1226 [==============================] - 2s 1ms/step - loss: 0.0503 - acc: 0.9869 - val_loss: 0.7573 - val_acc: 0.6656\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.64570 to 0.66556, saving model to Credit_Defaulter/weights-improvement-03-0.67.hdf5\n",
      "Epoch 4/10\n",
      "1226/1226 [==============================] - 2s 1ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.8272 - val_acc: 0.6722\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.66556 to 0.67219, saving model to Credit_Defaulter/weights-improvement-04-0.67.hdf5\n",
      "Epoch 5/10\n",
      "1226/1226 [==============================] - 2s 1ms/step - loss: 7.5728e-04 - acc: 1.0000 - val_loss: 0.8324 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.67219 to 0.67550, saving model to Credit_Defaulter/weights-improvement-05-0.68.hdf5\n",
      "Epoch 6/10\n",
      "1226/1226 [==============================] - 2s 1ms/step - loss: 4.4093e-04 - acc: 1.0000 - val_loss: 0.8481 - val_acc: 0.6722\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.67550\n",
      "Epoch 00006: early stopping\n",
      "[[223  81]\n",
      " [109 195]]\n",
      "\n",
      "\n",
      "Accuracy:  68.75\n",
      "\n",
      "\n",
      "Neural Network  4-fold cross validation 64.69\n"
     ]
    }
   ],
   "source": [
    "Neural_Network(X_OHE,y_labEnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.891417\n",
      "Will train until validation_0-auc hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-auc:0.91194\n",
      "[2]\tvalidation_0-auc:0.911173\n",
      "[3]\tvalidation_0-auc:0.911929\n",
      "[4]\tvalidation_0-auc:0.911885\n",
      "[5]\tvalidation_0-auc:0.912247\n",
      "[6]\tvalidation_0-auc:0.911874\n",
      "[7]\tvalidation_0-auc:0.912434\n",
      "[8]\tvalidation_0-auc:0.912488\n",
      "[9]\tvalidation_0-auc:0.913103\n",
      "[10]\tvalidation_0-auc:0.919233\n",
      "[11]\tvalidation_0-auc:0.918695\n",
      "[12]\tvalidation_0-auc:0.918202\n",
      "[13]\tvalidation_0-auc:0.918279\n",
      "[14]\tvalidation_0-auc:0.919748\n",
      "[15]\tvalidation_0-auc:0.919562\n",
      "[16]\tvalidation_0-auc:0.919721\n",
      "[17]\tvalidation_0-auc:0.920357\n",
      "[18]\tvalidation_0-auc:0.920872\n",
      "[19]\tvalidation_0-auc:0.91994\n",
      "[20]\tvalidation_0-auc:0.920905\n",
      "[21]\tvalidation_0-auc:0.920488\n",
      "[22]\tvalidation_0-auc:0.922512\n",
      "[23]\tvalidation_0-auc:0.922062\n",
      "[24]\tvalidation_0-auc:0.922062\n",
      "[25]\tvalidation_0-auc:0.92238\n",
      "[26]\tvalidation_0-auc:0.923077\n",
      "[27]\tvalidation_0-auc:0.922846\n",
      "[28]\tvalidation_0-auc:0.922956\n",
      "[29]\tvalidation_0-auc:0.923449\n",
      "[30]\tvalidation_0-auc:0.923647\n",
      "[31]\tvalidation_0-auc:0.924458\n",
      "[32]\tvalidation_0-auc:0.923702\n",
      "[33]\tvalidation_0-auc:0.923164\n",
      "[34]\tvalidation_0-auc:0.922802\n",
      "[35]\tvalidation_0-auc:0.923142\n",
      "[36]\tvalidation_0-auc:0.922868\n",
      "Stopping. Best iteration:\n",
      "[31]\tvalidation_0-auc:0.924458\n",
      "\n",
      "[[255  50]\n",
      " [ 58 247]]\n",
      "\n",
      "\n",
      "Accuracy:  82.3\n",
      "[0]\tvalidation_0-auc:0.856999\n",
      "Will train until validation_0-auc hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-auc:0.857152\n",
      "[2]\tvalidation_0-auc:0.858348\n",
      "[3]\tvalidation_0-auc:0.860607\n",
      "[4]\tvalidation_0-auc:0.868157\n",
      "[5]\tvalidation_0-auc:0.867411\n",
      "[6]\tvalidation_0-auc:0.868179\n",
      "[7]\tvalidation_0-auc:0.868036\n",
      "[8]\tvalidation_0-auc:0.866973\n",
      "[9]\tvalidation_0-auc:0.867565\n",
      "[10]\tvalidation_0-auc:0.867905\n",
      "[11]\tvalidation_0-auc:0.868168\n",
      "Stopping. Best iteration:\n",
      "[6]\tvalidation_0-auc:0.868179\n",
      "\n",
      "[[260  45]\n",
      " [ 48 257]]\n",
      "\n",
      "\n",
      "Accuracy:  84.75\n",
      "[0]\tvalidation_0-auc:0.859115\n",
      "Will train until validation_0-auc hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-auc:0.875998\n",
      "[2]\tvalidation_0-auc:0.876546\n",
      "[3]\tvalidation_0-auc:0.87489\n",
      "[4]\tvalidation_0-auc:0.875154\n",
      "[5]\tvalidation_0-auc:0.877413\n",
      "[6]\tvalidation_0-auc:0.878794\n",
      "[7]\tvalidation_0-auc:0.879858\n",
      "[8]\tvalidation_0-auc:0.886564\n",
      "[9]\tvalidation_0-auc:0.886707\n",
      "[10]\tvalidation_0-auc:0.884607\n",
      "[11]\tvalidation_0-auc:0.885166\n",
      "[12]\tvalidation_0-auc:0.882188\n",
      "[13]\tvalidation_0-auc:0.882978\n",
      "[14]\tvalidation_0-auc:0.881289\n",
      "Stopping. Best iteration:\n",
      "[9]\tvalidation_0-auc:0.886707\n",
      "\n",
      "[[249  56]\n",
      " [ 65 240]]\n",
      "\n",
      "\n",
      "Accuracy:  80.16\n",
      "[0]\tvalidation_0-auc:0.92674\n",
      "Will train until validation_0-auc hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-auc:0.932091\n",
      "[2]\tvalidation_0-auc:0.930863\n",
      "[3]\tvalidation_0-auc:0.928977\n",
      "[4]\tvalidation_0-auc:0.928363\n",
      "[5]\tvalidation_0-auc:0.928319\n",
      "[6]\tvalidation_0-auc:0.928407\n",
      "Stopping. Best iteration:\n",
      "[1]\tvalidation_0-auc:0.932091\n",
      "\n",
      "[[175 129]\n",
      " [ 19 285]]\n",
      "\n",
      "\n",
      "Accuracy:  75.66\n",
      "\n",
      "\n",
      " XGBoost 4-fold cross validation 80.7175\n"
     ]
    }
   ],
   "source": [
    "Xgboost(X_labEnc,y_labEnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.915797\n",
      "Will train until validation_0-auc hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-auc:0.915797\n",
      "[2]\tvalidation_0-auc:0.915797\n",
      "[3]\tvalidation_0-auc:0.915797\n",
      "[4]\tvalidation_0-auc:0.915797\n",
      "[5]\tvalidation_0-auc:0.929955\n",
      "[6]\tvalidation_0-auc:0.930891\n",
      "[7]\tvalidation_0-auc:0.930919\n",
      "[8]\tvalidation_0-auc:0.935098\n",
      "[9]\tvalidation_0-auc:0.937316\n",
      "[10]\tvalidation_0-auc:0.938087\n",
      "[11]\tvalidation_0-auc:0.937825\n",
      "[12]\tvalidation_0-auc:0.937502\n",
      "[13]\tvalidation_0-auc:0.938865\n",
      "[14]\tvalidation_0-auc:0.939375\n",
      "[15]\tvalidation_0-auc:0.937901\n",
      "[16]\tvalidation_0-auc:0.937942\n",
      "[17]\tvalidation_0-auc:0.936441\n",
      "[18]\tvalidation_0-auc:0.938004\n",
      "[19]\tvalidation_0-auc:0.936731\n",
      "Stopping. Best iteration:\n",
      "[14]\tvalidation_0-auc:0.939375\n",
      "\n",
      "[[317  80]\n",
      " [ 39 369]]\n",
      "\n",
      "\n",
      "Accuracy:  85.22\n",
      "[0]\tvalidation_0-auc:0.916281\n",
      "Will train until validation_0-auc hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-auc:0.92333\n",
      "[2]\tvalidation_0-auc:0.92333\n",
      "[3]\tvalidation_0-auc:0.9263\n",
      "[4]\tvalidation_0-auc:0.926218\n",
      "[5]\tvalidation_0-auc:0.926548\n",
      "[6]\tvalidation_0-auc:0.926218\n",
      "[7]\tvalidation_0-auc:0.931751\n",
      "[8]\tvalidation_0-auc:0.932785\n",
      "[9]\tvalidation_0-auc:0.933887\n",
      "[10]\tvalidation_0-auc:0.933419\n",
      "[11]\tvalidation_0-auc:0.932668\n",
      "[12]\tvalidation_0-auc:0.942074\n",
      "[13]\tvalidation_0-auc:0.94308\n",
      "[14]\tvalidation_0-auc:0.944265\n",
      "[15]\tvalidation_0-auc:0.945368\n",
      "[16]\tvalidation_0-auc:0.94738\n",
      "[17]\tvalidation_0-auc:0.948496\n",
      "[18]\tvalidation_0-auc:0.951211\n",
      "[19]\tvalidation_0-auc:0.951666\n",
      "[20]\tvalidation_0-auc:0.951446\n",
      "[21]\tvalidation_0-auc:0.951832\n",
      "[22]\tvalidation_0-auc:0.952493\n",
      "[23]\tvalidation_0-auc:0.9523\n",
      "[24]\tvalidation_0-auc:0.953568\n",
      "[25]\tvalidation_0-auc:0.952562\n",
      "[26]\tvalidation_0-auc:0.952769\n",
      "[27]\tvalidation_0-auc:0.952397\n",
      "[28]\tvalidation_0-auc:0.952218\n",
      "[29]\tvalidation_0-auc:0.951639\n",
      "Stopping. Best iteration:\n",
      "[24]\tvalidation_0-auc:0.953568\n",
      "\n",
      "[[344  56]\n",
      " [ 48 357]]\n",
      "\n",
      "\n",
      "Accuracy:  87.08\n",
      "[0]\tvalidation_0-auc:0.893392\n",
      "Will train until validation_0-auc hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-auc:0.893392\n",
      "[2]\tvalidation_0-auc:0.893392\n",
      "[3]\tvalidation_0-auc:0.894363\n",
      "[4]\tvalidation_0-auc:0.894363\n",
      "[5]\tvalidation_0-auc:0.894583\n",
      "[6]\tvalidation_0-auc:0.894377\n",
      "[7]\tvalidation_0-auc:0.894583\n",
      "[8]\tvalidation_0-auc:0.895093\n",
      "[9]\tvalidation_0-auc:0.895272\n",
      "[10]\tvalidation_0-auc:0.895272\n",
      "[11]\tvalidation_0-auc:0.895272\n",
      "[12]\tvalidation_0-auc:0.895493\n",
      "[13]\tvalidation_0-auc:0.900074\n",
      "[14]\tvalidation_0-auc:0.900088\n",
      "[15]\tvalidation_0-auc:0.900653\n",
      "[16]\tvalidation_0-auc:0.900433\n",
      "[17]\tvalidation_0-auc:0.898214\n",
      "[18]\tvalidation_0-auc:0.898366\n",
      "[19]\tvalidation_0-auc:0.900198\n",
      "[20]\tvalidation_0-auc:0.900102\n",
      "Stopping. Best iteration:\n",
      "[15]\tvalidation_0-auc:0.900653\n",
      "\n",
      "[[341  53]\n",
      " [ 68 343]]\n",
      "\n",
      "\n",
      "Accuracy:  84.97\n",
      "[0]\tvalidation_0-auc:0.854494\n",
      "Will train until validation_0-auc hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-auc:0.85353\n",
      "[2]\tvalidation_0-auc:0.853447\n",
      "[3]\tvalidation_0-auc:0.862333\n",
      "[4]\tvalidation_0-auc:0.870158\n",
      "[5]\tvalidation_0-auc:0.875406\n",
      "[6]\tvalidation_0-auc:0.874607\n",
      "[7]\tvalidation_0-auc:0.877514\n",
      "[8]\tvalidation_0-auc:0.879126\n",
      "[9]\tvalidation_0-auc:0.881027\n",
      "[10]\tvalidation_0-auc:0.882536\n",
      "[11]\tvalidation_0-auc:0.883734\n",
      "[12]\tvalidation_0-auc:0.88463\n",
      "[13]\tvalidation_0-auc:0.885594\n",
      "[14]\tvalidation_0-auc:0.884547\n",
      "[15]\tvalidation_0-auc:0.884864\n",
      "[16]\tvalidation_0-auc:0.887488\n",
      "[17]\tvalidation_0-auc:0.887185\n",
      "[18]\tvalidation_0-auc:0.887171\n",
      "[19]\tvalidation_0-auc:0.889472\n",
      "[20]\tvalidation_0-auc:0.890188\n",
      "[21]\tvalidation_0-auc:0.890361\n",
      "[22]\tvalidation_0-auc:0.895513\n",
      "[23]\tvalidation_0-auc:0.895981\n",
      "[24]\tvalidation_0-auc:0.896615\n",
      "[25]\tvalidation_0-auc:0.899701\n",
      "[26]\tvalidation_0-auc:0.899081\n",
      "[27]\tvalidation_0-auc:0.90039\n",
      "[28]\tvalidation_0-auc:0.901216\n",
      "[29]\tvalidation_0-auc:0.900955\n",
      "[30]\tvalidation_0-auc:0.900596\n",
      "[31]\tvalidation_0-auc:0.900851\n",
      "[32]\tvalidation_0-auc:0.900893\n",
      "[33]\tvalidation_0-auc:0.901609\n",
      "[34]\tvalidation_0-auc:0.901568\n",
      "[35]\tvalidation_0-auc:0.902036\n",
      "[36]\tvalidation_0-auc:0.907774\n",
      "[37]\tvalidation_0-auc:0.908931\n",
      "[38]\tvalidation_0-auc:0.910212\n",
      "[39]\tvalidation_0-auc:0.912513\n",
      "[40]\tvalidation_0-auc:0.913119\n",
      "[41]\tvalidation_0-auc:0.913236\n",
      "[42]\tvalidation_0-auc:0.913429\n",
      "[43]\tvalidation_0-auc:0.914283\n",
      "[44]\tvalidation_0-auc:0.914249\n",
      "[45]\tvalidation_0-auc:0.914662\n",
      "[46]\tvalidation_0-auc:0.915523\n",
      "[47]\tvalidation_0-auc:0.91542\n",
      "[48]\tvalidation_0-auc:0.91593\n",
      "[49]\tvalidation_0-auc:0.915971\n",
      "[50]\tvalidation_0-auc:0.916398\n",
      "[51]\tvalidation_0-auc:0.916343\n",
      "[52]\tvalidation_0-auc:0.917046\n",
      "[53]\tvalidation_0-auc:0.917293\n",
      "[54]\tvalidation_0-auc:0.917693\n",
      "[55]\tvalidation_0-auc:0.918051\n",
      "[56]\tvalidation_0-auc:0.91761\n",
      "[57]\tvalidation_0-auc:0.918341\n",
      "[58]\tvalidation_0-auc:0.917307\n",
      "[59]\tvalidation_0-auc:0.918134\n",
      "[60]\tvalidation_0-auc:0.91852\n",
      "[61]\tvalidation_0-auc:0.918582\n",
      "[62]\tvalidation_0-auc:0.918774\n",
      "[63]\tvalidation_0-auc:0.918712\n",
      "[64]\tvalidation_0-auc:0.919043\n",
      "[65]\tvalidation_0-auc:0.918919\n",
      "[66]\tvalidation_0-auc:0.919264\n",
      "[67]\tvalidation_0-auc:0.920063\n",
      "[68]\tvalidation_0-auc:0.919897\n",
      "[69]\tvalidation_0-auc:0.921103\n",
      "[70]\tvalidation_0-auc:0.920744\n",
      "[71]\tvalidation_0-auc:0.920703\n",
      "[72]\tvalidation_0-auc:0.920841\n",
      "[73]\tvalidation_0-auc:0.920924\n",
      "[74]\tvalidation_0-auc:0.920648\n",
      "Stopping. Best iteration:\n",
      "[69]\tvalidation_0-auc:0.921103\n",
      "\n",
      "[[342  58]\n",
      " [ 54 351]]\n",
      "\n",
      "\n",
      "Accuracy:  86.09\n",
      "[64.75, 67.870000000000005, 63.770000000000003, 67.599999999999994, 62.460000000000001, 65.25, 62.299999999999997, 68.75, 82.299999999999997, 84.75, 80.159999999999997, 75.659999999999997, 85.219999999999999, 87.079999999999998, 84.969999999999999, 86.090000000000003]\n",
      "\n",
      "\n",
      "XGBoost unstratified 4-fold cross validation 85.84\n"
     ]
    }
   ],
   "source": [
    "Xgboost_Unstratified(X_labEnc,y_labEnc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
